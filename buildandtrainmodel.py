# -*- coding: utf-8 -*-
"""buildAndTrainModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yAzR5XKcDUHqvLda-gqzCCgBnkRDViPn
"""

# Downloading necessary packages for this file
!pip install -q keras-tuner
!pip install keras-tuner --quiet

# Before running this file I have already imported 2 necessary files in Google Colab
# (dataset.tdrecords, customImputeLayerDefinition.py)

# Importing all necessary packages
import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras import layers, Model
import numpy as np
import importlib.util
import matplotlib.pyplot as plt
import shutil
from google.colab import files
import os
import random
import warnings
warnings.filterwarnings("ignore")

# Setting seed for reproducibility
SEED = 42
os.environ["PYTHONHASHSEED"] = str(SEED)
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Defining a parse_example method to decode TFRecord into input features and target
def parse_example(serialized_example):
    feature_description = {
        "tickers": tf.io.FixedLenFeature([188], tf.float32),
        "weekday": tf.io.FixedLenFeature([], tf.int64),
        "hour": tf.io.FixedLenFeature([], tf.int64),
        "month": tf.io.FixedLenFeature([], tf.int64),
        "target": tf.io.FixedLenFeature([], tf.int64),
    }
    parsed = tf.io.parse_single_example(serialized_example, feature_description)
    # Returning (feature_dict, target) for supervised training
    features = {k: parsed[k] for k in ["tickers", "weekday", "hour", "month"]}
    return features, parsed["target"]

# Loading and preprocessing the dataset from TFRecord
TFRECORD_FILE = "dataset.tfrecords"
raw_dataset = tf.data.TFRecordDataset(TFRECORD_FILE)
parsed_dataset = raw_dataset.map(parse_example).cache()  # Cache improves speed during repeated epochs

# Splitting the dataset into train/val/test using batching
BATCH_SIZE = 32
batched = parsed_dataset.batch(BATCH_SIZE)
total_batches = sum(1 for _ in batched)
n_train = int(0.7 * total_batches)
n_valid = int(0.15 * total_batches)
train_data = batched.take(n_train)
valid_data = batched.skip(n_train).take(n_valid)
test_data = batched.skip(n_train + n_valid)

# Creating input layers for each feature
# These are the input nodes for the functional API model
spec = importlib.util.spec_from_file_location("imputer_module", "customImputeLayerDefinition.py")
imputer_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(imputer_module)
ImputerLayer = imputer_module.ImputerLayer

# Initializing and adapting the ImputerLayer to train ticker data
imputer = ImputerLayer()
tickers_train = train_data.map(lambda x, y: x["tickers"])
imputer.adapt(tickers_train)

# Normalizing the imputed ticker values
imputed_tickers = tickers_train.map(lambda x: imputer(x))
normalizer = tf.keras.layers.Normalization()
normalizer.adapt(imputed_tickers)

# Defining custom loss with label smoothing to address overconfidence on imbalanced data
def smoothed_categorical_crossentropy(smoothing=0.1, num_classes=22):
    def loss_fn(y_true, y_pred):
        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)
        y_true_smoothed = y_true_one_hot * (1.0 - smoothing) + (smoothing / num_classes)
        return tf.keras.losses.categorical_crossentropy(y_true_smoothed, y_pred)
    return loss_fn

# Defining the model with hyperparameter search using keras_tuner
# Includes embedding for categorical vars, compressed ticker signal, and softmax output

def build_model(hp):
    act = hp.Choice("activation", ["relu", "elu", "selu"])
    num_layers = hp.Int("num_layers", 2, 4)
    lr = hp.Choice("learning_rate", [1e-2, 1e-3, 1e-4])
    batch_size = hp.Choice("batch_size", [32, 64, 128, 256])

    # Creating Keras Input Layers
    tickers_input = tf.keras.Input(shape=(188,), name="tickers")
    weekday_input = tf.keras.Input(shape=(), dtype=tf.int64, name="weekday")
    hour_input = tf.keras.Input(shape=(), dtype=tf.int64, name="hour")
    month_input = tf.keras.Input(shape=(), dtype=tf.int64, name="month")

    # Imputing and normalizing ticker signals + adding compression Dense layer
    x_tickers = normalizer(imputer(tickers_input))
    x_tickers = tf.keras.layers.Dense(64, activation="relu")(x_tickers)

    # Embedding categorical features and flattenning
    weekday_emb = tf.keras.layers.Embedding(7, 2)(weekday_input)
    hour_emb = tf.keras.layers.Embedding(24, 2)(hour_input)
    month_adjusted = tf.keras.layers.Lambda(lambda x: x - 1)(month_input)  # fixing month indexing
    month_emb = tf.keras.layers.Embedding(12, 2)(month_adjusted)

    # Concatenating all flattened features
    x = tf.keras.layers.Concatenate()([
        x_tickers,
        tf.keras.layers.Flatten()(weekday_emb),
        tf.keras.layers.Flatten()(hour_emb),
        tf.keras.layers.Flatten()(month_emb)
    ])

    # Adding tunable hidden dense layers with activation
    for i in range(num_layers):
        units = hp.Int(f'units_{i}', 64, 256, step=64)
        x = tf.keras.layers.Dense(units, activation=act)(x)

    # Final output layer: 22-way softmax for bin classification
    output = tf.keras.layers.Dense(22, activation="softmax")(x)

    # Defining full model from inputs to outputs
    model = tf.keras.Model(inputs=[tickers_input, weekday_input, hour_input, month_input], outputs=output)

    # Compiling with Adam optimizer and smoothed loss
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),
        loss=smoothed_categorical_crossentropy(smoothing=0.1, num_classes=22),
        metrics=["accuracy"]
    )
    return model

# Using RandomSearch to find best architecture and training params
# GridSearch was slow, RandomSearch is efficient enough here
# Creating tuner and search
tuner = kt.RandomSearch(
    build_model,
    objective="val_accuracy",
    max_trials=50,
    seed=SEED,  # <-- Add this
    directory="kt_final_randomsearch_dir",
    project_name="label_smoothing_compression"
)

# Fitting the model with early stopping on val_loss to avoid overfitting

# Training model
# Using keras_tuner to find optimal architecture

tuner.search(
    train_data,
    validation_data=valid_data,
    epochs=30,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

# Evaluating best model on test data
best_model = tuner.get_best_models(1)[0]
test_loss, test_acc = best_model.evaluate(test_data)
print(f"Final Test Accuracy: {test_acc * 100:.2f}%")

# Plotting training history for accuracy and loss
best_hps = tuner.get_best_hyperparameters(1)[0]
model = build_model(best_hps)
history = model.fit(
    train_data,
    validation_data=valid_data,
    epochs=30,
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

plt.figure(figsize=(10, 4))
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.title("Accuracy over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 4))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# Saving model in SavedModel format
best_model.export("mySavedModel")
shutil.make_archive("mySavedModel", 'zip', "mySavedModel")
files.download("mySavedModel.zip")